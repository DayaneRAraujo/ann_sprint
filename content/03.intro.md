# Introduction

Most scientific findings are communicated via scientific papers. In these papers knowledge is encoded in language, which makes it difficult for computers to search for facts. Additionally, millions of articles are produced every year, which makes it virtually impossible to keep track of current biomedical knowledge without the aid of computational tools. 

Annotation - linking words in scientific texts with external identifiers - is a ground step to automatically process biomedical knowledge. The annotation of biomedical articles is part of the work of biocurators, professionals dedicated to parse and make knowledge available on databases. These annotations, usually based on ontologies (sets of organized concepts)  already power core platforms used by the research community, such as ZFIN and UniProt. (Note that the meaning of "annotation" used here is different from sharing written notes about ideas on the text such as proposed by hypothes.is).

Even though  biomedical databases are extremely valuable, they are limited to specific subsets of human knowledge. It would be a colossal challenge to annotate text if we needed to look for the right ontology for each kind of concept. 


Wikidata is a possible solution for this challenge. It is knowledge base that contains more than 80 million varied concepts: “p53, “malaria”, “Douglas Adams”, “Brazil” and much more.  Moreover, anyone can contribute with new concepts (and relations between them) to Wikidata, which makes it flexible enough to accommodate the vast amount of concepts used in research articles.


During the 2-day hackathon eLife Sprint 2020, we started a project for annotating concepts in scientific articles to Wikidata, envisioning integration with Europe PMC’s Annotation API. We brainstormed both technical and practical aspects of developing a tool to gather crowd-annotations of scientific concepts. Inspired by other scientific games (like [Mark2Cure](https://mark2cure.org/), [eterna](https://eternagame.org/) and [fold.it](http://fold.it/)), we designed a gamified interface for crowdsourcing scientific annotations. Additionally, we studied Natural Language Processing approaches for extracting scientific entities, and assembled a series of perspectives on how to implement such an annotation tool in the current research environment.  

This document contains reports on the different branches of the project, coupled to thoughts of the participants on how to achieve the overarching goal of annotating all scientific text. 
Given the short time for the event, some parts of the report are not completely structured. Nevertheless, we believe that they can be useful for accessing the development of the project. 

### Community biocuration  projects

We note that this is not the first shot at organizing biomedical information by harnessing the power of the crowd. Some notable examples:

* PomBase (a base of information related to Schizosaccharomyces pombe) has a wonderful system for annotation articles to a set of OBO ontologies: https://github.com/pombase/canto [@doi:10.1093/database/baaa028]
* [UniProt has a community curation branch, which is under active development](http://insideuniprot.blogspot.com/2019/07/)
* Mark2Cure was a project with goals really similar to the Annotate Them All project, with some [very nice publications](https://mark2cure.org/blog/thank-you-campaign-pause/)  and a [GitHub page](https://github.com/SuLab/mark2cure/issues) [@doi:10.1093/bioinformatics/btz678]
* Cochrane has a crowd-annotation platform targeted at clinical trials: [Cochrane Crowd](https://crowd.cochrane.org/)


"Annotate them all" can be useful for these (and similar) projects, by providing coarse, community annotations for the dedicated, expert curator teams of bases such as UniProt and PomBase. 
