# Related softwares for annotation

The task of connecting mentions in scientific texts to identifiers in databases has been researched in both the biomedical and the natural language processing communities. There have been many approaches to automate this task. This is different from our human-in-the-loop approach. Nevertheless different components can help us to select candidates that we present to the annotators.

Of note, there have also been previous approaches for annotating documents using Wikipedia [@doi:10.1109/WIIAT.2008.56]

## Overview
-   Open Tapioca
    -   Recognizes entities and links them to Wikidata, but only for person, location organization
    -   Could be retrained with a subset of Wikidata that would contain only biomedical entities
-   Sci Spacy
    -   Entity detection and linking to UMLS ID
    -   Linked WikiText-2 Project
    -   Does Entity identification, Annotation with Wikidata entries.
    -   The project utilizes Stanford CoreNLP  
-   Doccano
    -   Does Entity identification, Sentiment analysis.
-   BERN
    -   Uses contextualized word embeddings, which might have higher accuracy than sci Spacy
    -   Does Entity Identification, Entity typing

### Open Tapioca

- [github repo](https://github.com/wetneb/opentapioca)
- [online demo](https://opentapioca.org/#)
- [paper](https://arxiv.org/pdf/1904.09131.pdf) [@https://arxiv.org/abs/1904.09131v1]
- [documentation](https://opentapioca.readthedocs.io/en/latest/)

#### System description 
Input is a sentence. Output is the sentence, with all persons, locations and organizations linked to their respective Wikidata identifier. The system is trained solely on Wikidata. The authors use occurrence statistics of concepts in Wikidata and in text to compute the likelihood of a certain word in the text linking to a certain Wikidata item (e.g. “Barack Obama” linking to “Q76”). To take context into account independently computed local features are propagated along a Markov chain. The authors claim that this system is lightweight and easy to retrain, and therefore easily adapts to the frequent changes of Wikidata. They say that restricting their system to only people, organizations and locations enabled them to do well without using any other data but Wikidata, while other approaches do rely on additional text from Wikipedia.

#### How feasible is this system for our project
Using only Wikidata to train the system is a good asset, because this might keep training times low. The authors are right in claiming that their system is lightweight: It does not use word embeddings or extensive language models but derives the necessary information about word similarities from Wikidata itself. The author states that this approach worked for relatively common entities, so it is unclear whether we can adapt it to less common biological entities. Testing the system on the cited website gave reasonably good results for less common names of people and cities, but was prone to misinterpret words that were not people, organization or locations as such (e.g. in the sentence “Banks are often closed.” the words “Banks” and “closed” were linked to locations). The documentation seems generally very good. 

#### Questions/Answers:

- How does the system pick out only people, locations and organizations? This is done before training, by using only entities of those categories in the training data set. (see documentation [here](https://opentapioca.readthedocs.io/en/latest/indexing.html))
- How easily can this be changed in the code? If we had a dump of Wikidata containing only biological entities we could use it to train on as described, no changes to the code itself needed! 

### Scispacy

- [github link](https://allenai.github.io/scispacy/)
- [online demo](https://scispacy.apps.allenai.org/)
- Article [@doi:10.18653/v1/W19-5034]

### System  description
The input is a sentence. The output is a sentence with the biomedical entities in that sentence annotated with canonical names, concept IDs and TUI(s). 

#### How feasible is this system for our project: 
This looks nearly perfect for candidate generation.

#### Questions:

- Are the IDs provided there in any way meaningful for linking them to Wikidata? Yes! (See section below) 

### Doccano

- [code base](https://github.com/doccano/doccano)
- [demo](http://doccano.herokuapp.com/)

Comments:
-   Entity identification; Sentiment analysis.

### Linked WikiText-2 Project

- [codebase](https://github.com/rloganiv/kglm-data)
- [demo](https://rloganiv.github.io/linked-wikitext-2/#/explore)
- [backbone](https://stanfordnlp.github.io/CoreNLP/corenlp-server.html)

Entity identification; Annotation with Wikidata entries; the project utilizes Stanford CoreNLP  



### BERN

- [code base](https://github.com/dmis-lab/bern)
- [demo](https://bern.korea.ac.kr/)

Uses contextualized word embeddings, which might have higher accuracy than sci Spacy
Does Entity Identification, Entity typing


## Applicability of the scispacy tool

The input for the software backend are abstracts of scientific articles that are loaded from Europe PMC using the Europe PMC API. We then use sci Spacy to detect entities in the abstract. Sci Spacy annotates those entities with their ID in the [Unified Medical Language System](https://www.nlm.nih.gov/research/umls/index.html) (UMLS) [@doi:10.1093/nar/gkh061].  

Notably, 26 thousand items in Wikidata have an UMLS ID, which allows to link the items that were detected by sciSpacy to be connected to Wikidata. We pinpoint a couple challenges: 

The pre-trained scispacy models are unable to identify the entity if it has conjunctions and prepositions in it. To improve the entity detection performance the model needs to be retrained using manually curated scientific word lists.
Sci spacy does not use contextualized word embeddings, which impacts the precision of retrieved entities (the model employed in sci Spacy is derived from this reference[@doi:10.18653/v1/N16-1030]).  

Other approaches use  contextualized word embeddings for detecting and normalizing biomedical entities (such as  https://bern.korea.ac.kr/).  They could be used in the project in  addition to scispacy.

Besides UMLS IDs, sciSpacy also can match concepts a number of MeSH IDs, which can also be linked to Wikidata items. 

We wrote code in a Google Colab notebook to concatenate the Europe PMC API with sciSpacy and Wikidata. After retrieving the abstract of an article via its PMID, the function extracts relevant concepts via sciSpacy and match the ones with PMIDs to Wikidata. 
The output of the pipeline is depicted in the Figure @fig:arup and the code is available in the [project github repository](https://github.com/lubianat/ann).

![ Concatenation of the Europe PMC API to Wikidata and sciSpacy
](images/arup.png){#fig:arup}
